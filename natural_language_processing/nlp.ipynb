{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "1053\n"
     ]
    }
   ],
   "source": [
    "# Brown corpus from NLTK\n",
    "print(brown.categories())\n",
    "\n",
    "data = brown.sents(categories='humor')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan told himself he would forget Annabelle.\n",
      "It was a very pleasent day. The weather was very cool.\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'Dan told himself he would forget Annabelle.'\n",
    "doc1 = '''It was a very pleasent day. The weather was very cool.'''\n",
    "\n",
    "print(sent1)\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan', 'told', 'himself', 'he', 'would', 'forget', 'Annabelle', '.']\n",
      "['It was a very pleasent day.', 'The weather was very cool.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentence into words\n",
    "print(word_tokenize(sent1))\n",
    "\n",
    "# Tokenize document into sentences\n",
    "print(sent_tokenize(doc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contact', 'me', 'at', 'xx', 'or', 'gaurav', 'xx', 'gmail', 'com']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using Regular Expression\n",
    "\n",
    "sent2 = 'Contact me at +91xx or gaurav001xx@gmail.com'\n",
    "regex = RegexpTokenizer('[a-zA-Z]+')\n",
    "\n",
    "print(regex.tokenize(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out', 'off', 'if', 'that', 'further', 'we', 'when', 'why', 's', 'be', 'wasn', 'which', 'than', 'now', \"aren't\", 'there', \"hasn't\", \"you've\", \"won't\", 'yourself', 'can', \"that'll\", 'because', 'aren', 'until', 'same', 'wouldn', 'very', 'shouldn', 'an', 'over', 'couldn', 'then', \"you're\", \"needn't\", 'how', 'most', 'haven', 'more', 'what', \"you'll\", 'down', 'won', 'has', 'only', 'under', 'so', 'just', 'needn', 'mustn', 'a', 'before', 'your', 'this', 'they', \"shan't\", 'while', 'with', 'whom', 'who', 'below', 'isn', 'into', 'once', 'on', 'each', 'having', 'their', 'no', 'd', \"isn't\", 'doesn', 'ourselves', 'his', 'is', 'am', 'are', 're', 'few', 'in', 'our', 'or', 'myself', 'above', 'll', 'ain', 'were', 'hers', 'm', 'my', 'had', 'from', 'of', 'y', 'not', 'hasn', 'both', 've', 'will', 'does', 'themselves', 'do', \"couldn't\", 'some', \"she's\", 'being', 'about', 'theirs', 'o', \"mightn't\", 'up', \"mustn't\", 'itself', 'he', 'such', 'yours', 'weren', 'ours', 'here', 'at', 'yourselves', \"should've\", \"hadn't\", 'don', \"shouldn't\", 'did', 'hadn', 'against', 'herself', \"doesn't\", 'she', 'it', 'mightn', \"wouldn't\", 'should', 'the', 'other', 'i', \"didn't\", 'doing', 'again', 'during', 'and', \"it's\", 'as', 'was', \"you'd\", 'after', 'between', 'where', 'for', \"wasn't\", 'shan', 'those', 'any', \"haven't\", 'by', 'its', 'her', \"don't\", 'them', 'own', 'nor', 'these', 'all', 'didn', 't', \"weren't\", 'have', 'been', 'ma', 'himself', 'but', 'too', 'me', 'to', 'him', 'you', 'through'}\n"
     ]
    }
   ],
   "source": [
    "# English Stopwords removal\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "print(sw)\n",
    "\n",
    "def remove_stopwords(words, stopwords):\n",
    "    useful_words = [w for w in words if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan', 'told', 'would', 'forget', 'Annabelle.']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(sent1.split(), sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "boy\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "ss = SnowballStemmer(language='english')\n",
    "print(ss.stem('jumping'))\n",
    "\n",
    "# Lemmatization\n",
    "wlm = WordNetLemmatizer()\n",
    "print(wlm.lemmatize('boyss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'wins': 39, 'world': 41, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'win': 38, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 40, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n",
      "[[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      "  0 2 0 1 0 2]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      "  0 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
      "  0 0 0 0 0 0]]\n",
      "(4, 42)\n"
     ]
    }
   ],
   "source": [
    "# Building a vocab and vectorization\n",
    "\n",
    "corpus = ['Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM.',\n",
    "        'The nobel laurate won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vect_corp = cv.fit_transform(corpus)\n",
    "\n",
    "print(cv.vocabulary_)\n",
    "print(vect_corp.toarray())\n",
    "print(vect_corp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['the', 'nobel', 'laurate', 'won', 'hearts', 'of', 'people'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "# Reverse mapping\n",
    "sent = cv.inverse_transform(vect_corp[2])\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 33)\n",
      "[array(['indian', 'cricket', 'team', 'wins', 'world', 'cup', 'says',\n",
      "       'capt', 'virat', 'kohli', 'held', 'sri', 'lanka'], dtype='<U9'), array(['indian', 'says', 'win', 'next', 'lok', 'sabha', 'elections',\n",
      "       'confident', 'pm'], dtype='<U9'), array(['nobel', 'laurate', 'hearts', 'people'], dtype='<U9'), array(['indian', 'movie', 'raazi', 'exciting', 'spy', 'thriller', 'based',\n",
      "       'upon', 'real', 'story'], dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "# Vectorization with custom tokenizer and stopword removal\n",
    "\n",
    "def myTokenizer(doc):\n",
    "    words = regex.tokenize(doc.lower())\n",
    "    words = remove_stopwords(words, sw)\n",
    "    return words\n",
    "\n",
    "cv2 = CountVectorizer(tokenizer=myTokenizer)\n",
    "vect_corp2 = cv2.fit_transform(corpus)\n",
    "\n",
    "print(vect_corp2.shape)\n",
    "print(cv2.inverse_transform(vect_corp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "test_corpus = ['Indian team rock!']\n",
    "out_corp = cv2.transform(test_corpus)\n",
    "print(out_corp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 8, 'is good': 2, 'good boy': 0, 'this is good': 9, 'is good boy': 3, 'is not': 4, 'not good': 6, 'good girl': 1, 'this is not': 10, 'is not good': 5, 'not good girl': 7}\n"
     ]
    }
   ],
   "source": [
    "# N-grams\n",
    "\n",
    "sents = ['This is good boy.', 'This is not good girl.']\n",
    "\n",
    "cv3 = CountVectorizer(ngram_range=(2, 3))\n",
    "vect_corp3 = cv3.fit_transform(sents)\n",
    "\n",
    "print(cv3.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 3, 'good': 2, 'boy': 0, 'not': 4, 'girl': 1}\n",
      "[[0.63009934 0.         0.44832087 0.44832087 0.         0.44832087]\n",
      " [0.         0.53309782 0.37930349 0.37930349 0.53309782 0.37930349]]\n"
     ]
    }
   ],
   "source": [
    "# Tf-Idf Normalization\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "vect_corp = tfidf.fit_transform(sents)\n",
    "\n",
    "print(tfidf.vocabulary_)\n",
    "print(vect_corp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
